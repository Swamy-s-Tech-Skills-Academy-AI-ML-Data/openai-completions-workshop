{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "379fa59e",
   "metadata": {},
   "source": [
    "# üöÄ Text Generation Apps using Python (Completions Only)\n",
    "\n",
    "This notebook showcases **14 text generation use cases** using OpenAI **Completions API**.  \n",
    "Each section includes a description, the Python code, and expected output.\n",
    "\n",
    "---\n",
    "\n",
    "## üìë Table of Contents\n",
    "1. [Setup](#setup)\n",
    "2. [Summarization](#uc1)  \n",
    "3. [Sentiment Classification](#uc2)  \n",
    "4. [Multilingual Generation + Translation](#uc3)  \n",
    "5. [Semantic Interpretation of Idiom](#uc4)  \n",
    "6. [Factual Recall / Explanatory Response](#uc5)  \n",
    "7. [Code Generation & Explanation](#uc6)  \n",
    "8. [Conversational Agent (FAQ Assistant)](#uc7)  \n",
    "9. [Style Transfer / Tone Adaptation](#uc8)  \n",
    "10. [Data-to-Text Generation](#uc9)  \n",
    "11. [Creative Writing](#uc10)  \n",
    "12. [Question Generation](#uc11)  \n",
    "13. [Entity Extraction with Explanation](#uc12)  \n",
    "14. [Paraphrasing / Rewriting](#uc13)  \n",
    "15. [Email / Document Drafting](#uc14)  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cfbf4a",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "## Setup\n",
    "\n",
    "- Ensure your `.env` file contains `OPENAI_API_KEY` (never hardcode secrets).\n",
    "- Use `python-dotenv` to load environment variables securely.\n",
    "- Import and initialize the `OpenAI` client with your API key.\n",
    "- Define a reusable helper function `get_completion(prompt, ...)` to call the OpenAI Completions API with configurable model, temperature, and max tokens.\n",
    "- Prefer environment variables for model selection and temperature overrides.\n",
    "- Example setup and usage are shown in the next code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1034991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (skip if already installed)\n",
    "# %pip install openai tiktoken python-dotenv\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Sequence\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI, APIError, RateLimitError\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Logging (avoid reconfiguring if handlers already exist)\n",
    "# -------------------------------------------------------------------\n",
    "if not logging.getLogger().handlers:\n",
    "    logging.basicConfig(level=logging.INFO,\n",
    "                        format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Environment loading & helpers\n",
    "# -------------------------------------------------------------------\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY is not set in environment or .env file\")\n",
    "\n",
    "DEFAULT_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-3.5-turbo-instruct\")\n",
    "\n",
    "\n",
    "def _env_float(name: str, default: float) -> float:\n",
    "    raw = os.getenv(name)\n",
    "    if not raw:\n",
    "        return default\n",
    "    # Ignore Windows TEMP style paths accidentally picked up\n",
    "    if os.path.sep in raw:\n",
    "        return default\n",
    "    try:\n",
    "        val = float(raw)\n",
    "        if 0 <= val <= 1:\n",
    "            return val\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return default\n",
    "\n",
    "\n",
    "def _env_int(name: str, default: int, min_val: int = 1, max_val: int = 4000) -> int:\n",
    "    raw = os.getenv(name)\n",
    "    if not raw:\n",
    "        return default\n",
    "    try:\n",
    "        val = int(raw)\n",
    "        if min_val <= val <= max_val:\n",
    "            return val\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return default\n",
    "\n",
    "\n",
    "def _get_client() -> OpenAI:\n",
    "    # Could be extended for per-request customization later\n",
    "    return OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "\n",
    "client = _get_client()\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Result container\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CompletionResult:\n",
    "    text: str\n",
    "    model: str\n",
    "    prompt_tokens: Optional[int] = None\n",
    "    completion_tokens: Optional[int] = None\n",
    "    total_tokens: Optional[int] = None\n",
    "    retries: int = 0\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Core helper with light retries\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def get_completion(\n",
    "    prompt: str,\n",
    "    model: str = DEFAULT_MODEL,\n",
    "    max_tokens: Optional[int] = None,\n",
    "    temperature: Optional[float] = None,\n",
    "    stop: Optional[Sequence[str]] = None,\n",
    "    log_response: bool = False,\n",
    "    max_retries: int = 3,\n",
    "    backoff_base: float = 1.5,\n",
    ") -> Optional[CompletionResult]:\n",
    "    \"\"\"Call OpenAI Completions API with resilience & env overrides.\n",
    "\n",
    "    Environment overrides:\n",
    "      OPENAI_MODEL          -> default model if not passed\n",
    "      OAI_TEMP / OAI_TEMPERATURE -> temperature (0..1)\n",
    "      OAI_MAX_TOKENS        -> max_tokens upper bound\n",
    "    \"\"\"\n",
    "    if not prompt or not prompt.strip():\n",
    "        raise ValueError(\"Prompt must be a non-empty string\")\n",
    "\n",
    "    # Apply env fallbacks only if caller did not specify\n",
    "    if temperature is None:\n",
    "        temperature = _env_float(\n",
    "            \"OAI_TEMP\", _env_float(\"OAI_TEMPERATURE\", 0.7))\n",
    "    if max_tokens is None:\n",
    "        max_tokens = _env_int(\"OAI_MAX_TOKENS\", 200, 1, 4096)\n",
    "\n",
    "    attempt = 0\n",
    "    last_error: Optional[Exception] = None\n",
    "    while attempt <= max_retries:\n",
    "        try:\n",
    "            start = time.time()\n",
    "            resp = client.completions.create(\n",
    "                model=model,\n",
    "                prompt=prompt.strip(),\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "                stop=list(stop) if stop else None,\n",
    "            )\n",
    "            latency = (time.time() - start) * 1000\n",
    "            choice = resp.choices[0]\n",
    "            usage = getattr(resp, \"usage\", None)\n",
    "            if log_response:\n",
    "                logger.debug(\"Raw response: %s\", resp)\n",
    "            logger.info(\n",
    "                \"completion model=%s tokens=%s latency=%.1fms retries=%d\",  # noqa: E501\n",
    "                model,\n",
    "                getattr(usage, \"total_tokens\", \"?\"),\n",
    "                latency,\n",
    "                attempt,\n",
    "            )\n",
    "            return CompletionResult(\n",
    "                text=choice.text.strip(),\n",
    "                model=model,\n",
    "                prompt_tokens=getattr(usage, \"prompt_tokens\", None),\n",
    "                completion_tokens=getattr(usage, \"completion_tokens\", None),\n",
    "                total_tokens=getattr(usage, \"total_tokens\", None),\n",
    "                retries=attempt,\n",
    "            )\n",
    "        except (RateLimitError, APIError) as e:\n",
    "            last_error = e\n",
    "            if attempt == max_retries:\n",
    "                logger.error(\"Max retries reached (%d): %s\", max_retries, e)\n",
    "                break\n",
    "            sleep_for = backoff_base ** attempt + random.uniform(0, 0.25)\n",
    "            logger.warning(\"Transient error (%s). Retrying in %.2fs (attempt %d/%d)\",\n",
    "                           e.__class__.__name__, sleep_for, attempt + 1, max_retries)\n",
    "            time.sleep(sleep_for)\n",
    "            attempt += 1\n",
    "            continue\n",
    "        except Exception as e:  # Non-retryable\n",
    "            logger.error(\"Completion failed: %s\", e)\n",
    "            last_error = e\n",
    "            break\n",
    "    return None\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Example usage (kept minimal for notebook demonstration)\n",
    "# -------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    test_prompt = \"Write a haiku about AI and the future.\"\n",
    "    result = get_completion(test_prompt, log_response=True)\n",
    "    if result:\n",
    "        print(\"‚úÖ Completion result:\\n\", result.text)\n",
    "        if result.total_tokens is not None:\n",
    "            print(\n",
    "                f\"(tokens: {result.total_tokens}, retries: {result.retries})\")\n",
    "    else:\n",
    "        print(\"‚ùå No response received.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221d9864",
   "metadata": {},
   "source": [
    "<a id=\"uc1\"></a>\n",
    "## ‚úÖ Use Case 1: Summarization\n",
    "**Input:**  \n",
    "Sparse vector feature indexing allows scalable search over events by turning categorical attributes into an extremely wide, mostly empty vector...\n",
    "\n",
    "**Sample Output:**  \n",
    "\"It converts categorical features into sparse vectors for scalable search. The trade-off is efficiency vs. memory/compute cost at high cardinality.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2904a9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Use Case 1: Summarization\n",
    "\n",
    "technical_paragraph = \"\"\"\n",
    "Sparse vector feature indexing allows scalable search over events by turning \n",
    "categorical attributes into an extremely wide, mostly empty vector. Each distinct \n",
    "token (e.g. event_type:click, browser:mobile) maps to a coordinate set to 1 while \n",
    "all others remain 0. This removes unintended ordering between categories and keeps \n",
    "distance metrics meaningful, but at very high cardinality memory and compute \n",
    "overhead grow rapidly, motivating hashing tricks or learned embeddings.\n",
    "\"\"\".strip()\n",
    "\n",
    "summary_prompt = f\"Summarize the following paragraph in 2 crisp sentences focusing on purpose and trade-offs:\\n\\n{technical_paragraph}\\n\\nSummary:\"\n",
    "\n",
    "# Call completions API using helper\n",
    "summary = get_completion(\n",
    "    summary_prompt, max_tokens=120, temperature=0.4)\n",
    "\n",
    "print(\"‚úÖ Summary:\", summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c7a222",
   "metadata": {},
   "source": [
    "<a id=\"uc2\"></a>\n",
    "## ‚úÖ Use Case 2: Sentiment Classification\n",
    "**Input:**  \n",
    "\"The user interface feels sluggish, but the reporting features are fantastic.\"\n",
    "\n",
    "**Sample Output:**  \n",
    "\"Neutral\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e3576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Use Case 2: Sentiment Classification (Multiple Inputs)\n",
    "\n",
    "# Sample inputs\n",
    "input_texts = [\n",
    "    \"The user interface feels sluggish, but the reporting features are fantastic.\",  # Neutral\n",
    "    \"I love the new dashboard design, it‚Äôs clean and very intuitive.\",               # Positive\n",
    "    \"The system keeps crashing frequently, making it hard to use.\"                   # Negative\n",
    "]\n",
    "\n",
    "# Loop through each input and classify sentiment\n",
    "for input_text in input_texts:\n",
    "    sentiment_prompt = f\"\"\"\n",
    "    Classify the sentiment of the following text strictly as Positive, Negative, or Neutral.\n",
    "    If the text contains both positive and negative aspects, classify it as Neutral.\n",
    "\n",
    "    Text: \"{input_text}\"\n",
    "\n",
    "    Sentiment:\n",
    "    \"\"\"\n",
    "\n",
    "    # Call completions API using helper (returns CompletionResult or None)\n",
    "    result = get_completion(\n",
    "        sentiment_prompt,\n",
    "        max_tokens=20,\n",
    "        temperature=0  # deterministic output\n",
    "    )\n",
    "\n",
    "    if result and result.text:\n",
    "        sentiment_text = result.text.strip()\n",
    "    else:\n",
    "        sentiment_text = \"(no response)\"\n",
    "\n",
    "    print(f\"‚úÖ Text: {input_text}\")\n",
    "    print(f\"‚úÖ Sentiment: {sentiment_text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c6fc11",
   "metadata": {},
   "source": [
    "<a id=\"uc3\"></a>\n",
    "## ‚úÖ Use Case 3: Multilingual Generation + Translation\n",
    "**Input:**  \n",
    "\"Artificial Intelligence will transform education.\"\n",
    "\n",
    "**Sample Output:**  \n",
    "- French: \"L'intelligence artificielle transformera l'√©ducation.\"  \n",
    "- Spanish: \"La inteligencia artificial transformar√° la educaci√≥n.\"  \n",
    "- Hindi: \"‡§ï‡•É‡§§‡•ç‡§∞‡§ø‡§Æ ‡§¨‡•Å‡§¶‡•ç‡§ß‡§ø‡§Æ‡§§‡•ç‡§§‡§æ ‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ ‡§ï‡•ã ‡§¨‡§¶‡§≤ ‡§¶‡•á‡§ó‡•Ä‡•§\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56b1e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Use Case 3: Multilingual Generation + Translation\n",
    "\n",
    "input_text = \"Artificial Intelligence will transform education.\"\n",
    "languages = [\"French\", \"Spanish\", \"Hindi\"]\n",
    "\n",
    "for lang in languages:\n",
    "    translation_prompt = f\"Translate the following text into {lang}:\\n\\n{input_text}\"\n",
    "    translation = get_completion(\n",
    "        translation_prompt, max_tokens=60, temperature=0.3)\n",
    "    print(f\"‚úÖ {lang}: {translation}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1177ce",
   "metadata": {},
   "source": [
    "<a id=\"uc4\"></a>\n",
    "## ‚úÖ Use Case 4: Semantic Interpretation of Idiom\n",
    "**Input:**  \n",
    "\"Kick the bucket\"\n",
    "\n",
    "**Sample Output:**  \n",
    "\"It means someone has died.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae68ab33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Use Case 4: Semantic Interpretation of Idiom\n",
    "\n",
    "idioms = [\n",
    "    \"kick the bucket\",\n",
    "    \"spill the beans\",\n",
    "    \"hit the sack\",\n",
    "    \"once in a blue moon\",\n",
    "    \"break the ice\",\n",
    "]\n",
    "\n",
    "for idiom in idioms:\n",
    "    prompt = f\"\"\"\n",
    "Explain the meaning of the idiom below in one concise, literal-friendly plain English sentence.\n",
    "Avoid extra commentary or cultural notes unless essential.\n",
    "Return ONLY the meaning (no leading labels).\n",
    "\n",
    "Idiom: \"{idiom}\"\n",
    "\n",
    "Meaning:\n",
    "\"\"\".strip()\n",
    "\n",
    "    meaning = get_completion(\n",
    "        prompt,\n",
    "        model=\"gpt-3.5-turbo-instruct\",\n",
    "        max_tokens=60,\n",
    "        temperature=0  # deterministic for stable definitions\n",
    "    )\n",
    "\n",
    "    if meaning:\n",
    "        print(f\"üó£Ô∏è {idiom} -> {meaning}\\n\\n\")\n",
    "    else:\n",
    "        print(f\"‚ùå No response for idiom: {idiom}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450e8e44",
   "metadata": {},
   "source": [
    "<a id=\"uc5\"></a>\n",
    "## ‚úÖ Use Case 5: Factual Recall / Explanatory Response\n",
    "**Input:**  \n",
    "\"Why is the sky blue?\"\n",
    "\n",
    "**Sample Output:**  \n",
    "\"The sky looks blue because sunlight is scattered by air molecules. Blue light is scattered more strongly than other colors, so we mostly see blue.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6538f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Use Case 5: Factual Recall / Explanatory Response\n",
    "\n",
    "question = \"Why is the sky blue?\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Provide a concise scientific explanation (2 sentences) for the question below.\n",
    "Avoid fluff; focus on the underlying physical phenomenon.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "result = get_completion(prompt, max_tokens=120, temperature=0.2)\n",
    "\n",
    "if result and result.text:\n",
    "    print(\"‚ùì Question:\", question)\n",
    "    print(\"üß™ Answer:\", result.text.strip())\n",
    "    if result.total_tokens is not None:\n",
    "        print(f\"(tokens: {result.total_tokens}, retries: {result.retries})\")\n",
    "else:\n",
    "    print(\"‚ùå No response received.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3663aa09",
   "metadata": {},
   "source": [
    "<a id=\"uc6\"></a>\n",
    "## ‚úÖ Use Case 6: Code Generation & Explanation\n",
    "**Input:**  \n",
    "\"Write a Python function that checks if a number is prime.\"\n",
    "\n",
    "**Sample Output:**  \n",
    "```python\n",
    "def is_prime(n):\n",
    "    if n < 2:\n",
    "        return False\n",
    "    for i in range(2, int(n**0.5) + 1):\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "    return True\n",
    "````\n",
    "\n",
    "**Explanation:**\n",
    "\"The function checks divisibility from 2 up to ‚àön. If any divisor is found, it returns False; otherwise, True.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e8cd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Use Case 6: Code Generation & Explanation\n",
    "\n",
    "problem = \"Write a Python function that checks if a number is prime.\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You are a senior Python engineer.\n",
    "Generate clean, production-quality code that fulfills the requirement below.\n",
    "After the code, add a single concise explanation sentence.\n",
    "Use exactly this delimiter line before the explanation:\n",
    "---EXPLANATION---\n",
    "\n",
    "Requirement:\n",
    "{problem}\n",
    "\n",
    "Return ONLY:\n",
    "1. A single Python code block\n",
    "2. The delimiter line\n",
    "3. One concise explanation sentence\n",
    "\"\"\".strip()\n",
    "\n",
    "result = get_completion(prompt, max_tokens=220, temperature=0)\n",
    "\n",
    "if not result or not result.text:\n",
    "    print(\"‚ùå No response received.\")\n",
    "else:\n",
    "    raw = result.text.strip()\n",
    "    # Attempt to split code and explanation\n",
    "    parts = raw.split(\"---EXPLANATION---\", 1)\n",
    "    code_part = raw\n",
    "    explanation = None\n",
    "    if len(parts) == 2:\n",
    "        code_part, explanation = parts[0].strip(), parts[1].strip()\n",
    "\n",
    "    print(\"üß© Generated Code:\")\n",
    "    print(code_part)\n",
    "    if explanation:\n",
    "        print(\"\\nüìù Explanation:\", explanation)\n",
    "    if result.total_tokens is not None:\n",
    "        print(f\"\\n(tokens: {result.total_tokens}, retries: {result.retries})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e6919f",
   "metadata": {},
   "source": [
    "<a id=\"uc7\"></a>\n",
    "## ‚úÖ Use Case 7: Conversational Agent (FAQ Assistant)\n",
    "\n",
    "**Input:**\n",
    "\"What is the purpose of Git?\"\n",
    "\n",
    "**Sample Output:**\n",
    "\"Git is a version control system that tracks changes in code, helps collaboration, and manages project history.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0f6c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Use Case 7: Conversational Agent (FAQ Assistant)\n",
    "\n",
    "# A tiny in-memory FAQ knowledge base (could be loaded from file/db in real apps)\n",
    "faq_kb = {\n",
    "    \"What is Git?\": \"Git is a distributed version control system for tracking changes in source code.\",\n",
    "    \"Why use version control?\": \"It enables history, collaboration, branching, and safe experimentation.\",\n",
    "    \"What is a commit?\": \"A commit records a snapshot of your staged changes with a message.\",\n",
    "    \"What is branching?\": \"Branching lets you diverge from the main line of development to work independently.\",\n",
    "    \"What is a pull request?\": \"A pull request proposes merging changes from one branch into another, supporting review.\",\n",
    "}\n",
    "\n",
    "# Simulated user questions (some exact, some paraphrased / partially overlapping)\n",
    "user_queries = [\n",
    "    \"What is the purpose of Git?\",\n",
    "    \"Explain branching in simple terms.\",\n",
    "    \"Why should teams adopt version control?\",\n",
    "]\n",
    "\n",
    "system_instruction = (\n",
    "    \"You are a concise technical FAQ assistant. Answer ONLY from the provided FAQ entries. \"\n",
    "    \"If the answer is not present, reply with 'I don't have that information.' Keep answers to one or two sentences.\"\n",
    ")\n",
    "\n",
    "history: list[tuple[str, str]] = []  # (user_question, answer)\n",
    "\n",
    "for q in user_queries:\n",
    "    # Build a retrieval-aware prompt by listing KB and prior turns.\n",
    "    kb_block = \"\\n\".join(f\"Q: {k}\\nA: {v}\" for k, v in faq_kb.items())\n",
    "    convo_block = \"\\n\".join(\n",
    "        # last few turns\n",
    "        f\"User: {hq}\\nAssistant: {ha}\" for hq, ha in history[-4:])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "{system_instruction}\n",
    "\n",
    "FAQ Knowledge Base:\n",
    "{kb_block}\n",
    "\n",
    "Conversation So Far:\n",
    "{convo_block if convo_block else '(no previous turns)'}\n",
    "\n",
    "User Question: {q}\n",
    "\n",
    "Answer (draw ONLY from KB or say you don't have it):\n",
    "\"\"\".strip()\n",
    "\n",
    "    result = get_completion(prompt, max_tokens=110, temperature=0.2)\n",
    "\n",
    "    if result and result.text:\n",
    "        answer = result.text.strip()\n",
    "    else:\n",
    "        answer = \"(no response)\"\n",
    "\n",
    "    history.append((q, answer))\n",
    "    print(f\"üôã User: {q}\")\n",
    "    print(f\"ü§ñ Assistant: {answer}\\n\")\n",
    "\n",
    "# Show a compact transcript summary\n",
    "print(\"--- Transcript Summary ---\")\n",
    "for uq, ans in history:\n",
    "    print(f\"Q: {uq}\\nA: {ans}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7da3675",
   "metadata": {},
   "source": [
    "\n",
    "## ‚úÖ Use Case 8: Style Transfer / Tone Adaptation\n",
    "\n",
    "**Input:**\n",
    "\"Please confirm receipt of this document at the earliest.\"\n",
    "\n",
    "**Sample Output:**\n",
    "\"Hey, just let me know when you get this doc!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab64e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Use Case 8: Style Transfer / Tone Adaptation\n",
    "\n",
    "original_text = \"Please confirm receipt of this document at the earliest.\"\n",
    "\n",
    "# Define target tones / style adaptation goals\n",
    "style_specs = [\n",
    "    {\n",
    "        \"label\": \"Casual Friendly\",\n",
    "        \"instruction\": \"Rewrite to a casual, friendly tone as if messaging a colleague you know well. Conserve meaning.\"\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Professional Polite\",\n",
    "        \"instruction\": \"Rewrite in a concise, professional, polite corporate email tone. Avoid urgency exaggeration.\"\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Enthusiastic Marketing\",\n",
    "        \"instruction\": \"Rewrite with upbeat, energetic marketing-style enthusiasm while keeping the core request intact.\"\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Concise SMS\",\n",
    "        \"instruction\": \"Rewrite as a very short SMS text (‚â§ 14 words), plain language, no emojis.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Temperature per style (defaults if not specified)\n",
    "style_temperature = {\n",
    "    \"Casual Friendly\": 0.6,\n",
    "    \"Professional Polite\": 0.2,\n",
    "    \"Enthusiastic Marketing\": 0.8,\n",
    "    \"Concise SMS\": 0.3,\n",
    "}\n",
    "\n",
    "print(f\"üìÑ Original: {original_text}\\n\")\n",
    "\n",
    "for spec in style_specs:\n",
    "    label = spec[\"label\"]\n",
    "    instruction = spec[\"instruction\"]\n",
    "    temperature = style_temperature.get(label, 0.5)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an expert tone adaptation assistant.\n",
    "Task: {instruction}\n",
    "\n",
    "Constraints:\n",
    "- Preserve the original factual meaning.\n",
    "- Do NOT add new details, placeholders, or emojis unless specified.\n",
    "- Output ONLY the rewritten sentence (no quotes, no label, no explanation).\n",
    "\n",
    "Original: {original_text}\n",
    "\n",
    "Rewritten:\n",
    "\"\"\".strip()\n",
    "\n",
    "    result = get_completion(prompt, max_tokens=60, temperature=temperature)\n",
    "\n",
    "    if result and result.text:\n",
    "        print(f\"üéØ {label}: {result.text.strip()}\\n\\n\")\n",
    "    else:\n",
    "        print(f\"‚ùå {label}: (no response)\\n\\n\")\n",
    "\n",
    "# (Optional) You could adapt this into a function or batch process for many sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb220da",
   "metadata": {},
   "source": [
    "## ‚úÖ Use Case 9: Data-to-Text Generation\n",
    "\n",
    "**Input:**\n",
    "Region: North\n",
    "Sales: \\$50,000\n",
    "Increase: 10%\n",
    "\n",
    "**Sample Output:**\n",
    "\"The North region generated \\$50,000 in sales, reflecting a 10% increase compared to the previous period.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0163da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Use Case 9: Data-to-Text Generation\n",
    "\"\"\"Generate natural language summaries from structured tabular data.\n",
    "This demonstrates:\n",
    "- Turning rows into fluent sentences\n",
    "- Enforcing style & constraints\n",
    "- Producing both per-row narratives and an aggregate insight\n",
    "\"\"\"\n",
    "\n",
    "# Example structured data (could come from a dataframe / CSV)\n",
    "# Amounts in USD\n",
    "regional_metrics = [\n",
    "    {\"region\": \"North\", \"sales\": 50000, \"growth_pct\": 10.0, \"new_customers\": 42},\n",
    "    {\"region\": \"South\", \"sales\": 38000, \"growth_pct\": 4.5, \"new_customers\": 35},\n",
    "    {\"region\": \"West\",  \"sales\": 61000, \"growth_pct\": 12.3, \"new_customers\": 57},\n",
    "    {\"region\": \"East\",  \"sales\": 45500, \"growth_pct\": 7.1, \"new_customers\": 48},\n",
    "]\n",
    "\n",
    "# Helper to format data block for the prompt (kept deterministic)\n",
    "\n",
    "\n",
    "def format_data(rows: list[dict]) -> str:\n",
    "    lines = [\"Region | Sales | GrowthPct | NewCustomers\"]\n",
    "    for r in rows:\n",
    "        lines.append(\n",
    "            f\"{r['region']} | {r['sales']} | {r['growth_pct']} | {r['new_customers']}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "data_block = format_data(regional_metrics)\n",
    "\n",
    "per_row_instruction = (\n",
    "    \"For each region produce ONE sentence: 'The <Region> region generated $X in sales (Y% growth) adding Z new customers.' \"\n",
    "    \"Use commas sparingly. No extra commentary.\"\n",
    ")\n",
    "\n",
    "aggregate_instruction = (\n",
    "    \"After listing sentences, add a final concise aggregate insight comparing best vs weakest growth and overall momentum. \"\n",
    "    \"Do NOT restate raw table values verbatim beyond what is needed.\"\n",
    ")\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You are a data summarization assistant.\n",
    "Transform the structured table into natural language.\n",
    "\n",
    "Table (pipe-delimited):\n",
    "{data_block}\n",
    "\n",
    "Instructions:\n",
    "- {per_row_instruction}\n",
    "- {aggregate_instruction}\n",
    "- Keep total output under 8 sentences.\n",
    "- Tone: objective, analyst brief.\n",
    "- Output only the sentences (no bullets, no headings).\n",
    "\n",
    "Output:\n",
    "\"\"\".strip()\n",
    "\n",
    "result = get_completion(prompt, max_tokens=260, temperature=0.25)\n",
    "\n",
    "if not result or not result.text:\n",
    "    print(\"‚ùå No response received.\")\n",
    "else:\n",
    "    print(\"üìù Data-to-Text Summary:\\n\")\n",
    "    print(result.text.strip())\n",
    "    if result.total_tokens is not None:\n",
    "        print(f\"\\n(tokens: {result.total_tokens}, retries: {result.retries})\")\n",
    "\n",
    "# Optional: show how you might adapt a single-row quick narrative\n",
    "\n",
    "\n",
    "def single_row_narrative(row: dict) -> str:\n",
    "    p = (\n",
    "        \"Rewrite the following structured record as one concise sentence summarizing performance. \"\n",
    "        \"Use present tense. No embellishment. Return only the sentence.\"  # deterministic pattern\n",
    "    )\n",
    "    record_block = \", \".join(f\"{k}={v}\" for k, v in row.items())\n",
    "    single_prompt = f\"{p}\\nRecord: {record_block}\\nSentence:\"\n",
    "    single_res = get_completion(single_prompt, max_tokens=50, temperature=0.2)\n",
    "    return single_res.text.strip() if single_res and single_res.text else \"(no result)\"\n",
    "\n",
    "\n",
    "print(\"\\n--- Single Row Example (West) ---\")\n",
    "print(single_row_narrative(regional_metrics[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f58db56",
   "metadata": {},
   "source": [
    "## ‚úÖ Use Case 10: Creative Writing\n",
    "\n",
    "**Input:**\n",
    "\"A robot learning to paint\"\n",
    "\n",
    "**Sample Output:**\n",
    "\"The robot dipped its brush in bright colors, each stroke a clumsy attempt at beauty. Slowly, it learned to create sunsets and flowers. Its creators realized the machine had found its soul in art.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90662f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Use Case 10: Creative Writing\n",
    "\"\"\"Generate multiple stylistic story variants from the same seed concept.\n",
    "Demonstrates:\n",
    "- Controlling style & tone via instructions\n",
    "- Adjusting creativity using temperature\n",
    "- Lightweight post-processing (title suggestion)\n",
    "\"\"\"\n",
    "\n",
    "story_seed = \"A robot learning to paint\"\n",
    "\n",
    "style_specs = [\n",
    "    {\n",
    "        \"label\": \"Concise Vignette\",\n",
    "        \"instruction\": \"Write 3 crisp sentences, understated and cinematic. Subtle emotion, no dialogue.\",\n",
    "        \"temperature\": 0.65,\n",
    "        \"max_tokens\": 160,\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Descriptive Sci‚ÄëFi\",\n",
    "        \"instruction\": \"Write 4 sentences with tactile sensory detail and soft speculative imagery. Mild wonder, no techno-babble.\",\n",
    "        \"temperature\": 0.85,\n",
    "        \"max_tokens\": 190,\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Whimsical Children's Book\",\n",
    "        \"instruction\": \"Write 3‚Äì4 short, playful sentences a child can understand. Gentle personification, warm ending.\",\n",
    "        \"temperature\": 0.9,\n",
    "        \"max_tokens\": 170,\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Reflective Literary\",\n",
    "        \"instruction\": \"Write one compact paragraph (4 sentences) with lyrical cadence and a quiet epiphany in the final line.\",\n",
    "        \"temperature\": 0.8,\n",
    "        \"max_tokens\": 210,\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"üé® Seed Concept: {story_seed}\\n\")\n",
    "all_variants: list[tuple[str, str]] = []  # (label, text)\n",
    "\n",
    "for spec in style_specs:\n",
    "    label = spec[\"label\"]\n",
    "    instruction = spec[\"instruction\"]\n",
    "    temp = spec[\"temperature\"]\n",
    "    max_toks = spec[\"max_tokens\"]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an accomplished creative writer.\n",
    "Task: {instruction}\n",
    "Seed Concept: \"{story_seed}\"\n",
    "Constraints:\n",
    "- Avoid clich√©s (e.g., 'outside the box', 'glitch in the system').\n",
    "- Keep the passage self-contained (no need for title).\n",
    "- No explicit moral statements.\n",
    "- Return ONLY the passage (no label, no meta commentary).\n",
    "\n",
    "Passage:\n",
    "\"\"\".strip()\n",
    "\n",
    "    result = get_completion(prompt, max_tokens=max_toks, temperature=temp)\n",
    "\n",
    "    if not result or not result.text:\n",
    "        print(f\"‚ùå {label}: (no response)\\n\")\n",
    "        continue\n",
    "\n",
    "    text = result.text.strip()\n",
    "    all_variants.append((label, text))\n",
    "    print(f\"üñãÔ∏è {label} (temp={temp}):\\n{text}\\n\")\n",
    "\n",
    "# Optional: Generate a concise evocative title for the strongest variant (choose the Reflective Literary if present)\n",
    "reflective = next(\n",
    "    (t for (lbl, t) in all_variants if lbl.startswith(\"Reflective\")), None)\n",
    "if reflective:\n",
    "    title_prompt = f\"Provide a 5-word evocative literary title (Title Case) for the following passage. No quotes.\\n\\nPassage:\\n{reflective}\\n\\nTitle:\".strip(\n",
    "    )\n",
    "    title_res = get_completion(title_prompt, max_tokens=12, temperature=0.6)\n",
    "    if title_res and title_res.text:\n",
    "        print(\"üè∑Ô∏è Suggested Title:\", title_res.text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd24c6a",
   "metadata": {},
   "source": [
    "## ‚úÖ Use Case 11: Question Generation\n",
    "\n",
    "**Input:**\n",
    "\"The Pacific Ocean is the largest and deepest of Earth's oceanic divisions. It extends from the Arctic Ocean in the north to the Southern Ocean in the south.\"\n",
    "\n",
    "**Sample Output:**\n",
    "\n",
    "1. What is the largest ocean on Earth?\n",
    "2. How deep is the Pacific Ocean compared to other oceans?\n",
    "3. Which oceans border the Pacific to the north and south?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b544c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Use Case 11: Question Generation\n",
    "\"\"\"Generate study / assessment questions from a source passage.\n",
    "Demonstrates:\n",
    "- Mapping a single text to multiple cognitive levels (factual, inferential, analytical)\n",
    "- Controlling format and difficulty tiers\n",
    "- Light post-processing to ensure clean numbering\n",
    "\"\"\"\n",
    "\n",
    "source_passage = (\n",
    "    \"The Pacific Ocean is the largest and deepest of Earth's oceanic divisions. \"\n",
    "    \"It stretches from the Arctic in the north to the Southern Ocean near Antarctica in the south, \"\n",
    "    \"and is bounded by Asia and Australia in the west and the Americas in the east. Its vast size \"\n",
    "    \"influences global climate patterns, marine biodiversity, and trade routes.\"\n",
    ")\n",
    "\n",
    "# Specification for desired questions\n",
    "question_plan = [\n",
    "    (\"Factual Recall\", 2, \"Direct facts explicitly stated (who / what / where).\"),\n",
    "    (\"Conceptual / Inferential\", 2,\n",
    "     \"Questions requiring connecting two statements or reasoning implicit relationships.\"),\n",
    "    (\"Analytical / Higher-Order\", 1,\n",
    "     \"Question prompting implications, significance, or broader impact.\"),\n",
    "]\n",
    "\n",
    "\n",
    "def build_instruction(plan):\n",
    "    lines = [\"Generate high-quality questions from the passage.\", \"Rules:\"]\n",
    "    lines += [\n",
    "        \"- No answers in output.\",\n",
    "        \"- Each question must be self-contained (can stand alone).\",\n",
    "        \"- Avoid redundancy and avoid yes/no questions.\",\n",
    "        \"- Use clear academic English.\",\n",
    "        \"- Do NOT mention the tiers in the output; just list questions numbered.\",\n",
    "    ]\n",
    "    lines.append(\"Desired distribution (internal guide, not to print):\")\n",
    "    for tier, count, desc in plan:\n",
    "        lines.append(f\"  * {tier}: {count} ‚Äì {desc}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "instruction_block = build_instruction(question_plan)\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You are an educational content designer.\n",
    "Passage:\\n{source_passage}\\n\\n{instruction_block}\\n\\nOutput ONLY numbered questions (1., 2., ...). Start numbering at 1.\n",
    "Questions:\n",
    "\"\"\".strip()\n",
    "\n",
    "result = get_completion(prompt, max_tokens=220, temperature=0.3)\n",
    "\n",
    "if not result or not result.text:\n",
    "    print(\"‚ùå No response received.\")\n",
    "else:\n",
    "    raw = result.text.strip()\n",
    "    # Post-process: ensure numbering starts at 1 and is sequential\n",
    "    lines = [l.strip() for l in raw.splitlines() if l.strip()]\n",
    "    cleaned = []\n",
    "    counter = 1\n",
    "    for ln in lines:\n",
    "        # Remove existing leading number patterns like '1.' or '1)' or '(1)'\n",
    "        stripped = ln\n",
    "        # Simple patterns\n",
    "        if stripped[0:3].isdigit():\n",
    "            pass  # rare case; handle generically below\n",
    "        import re\n",
    "        stripped = re.sub(r\"^\\(?\\d+[\\).]\\s*\", \"\", stripped)\n",
    "        cleaned.append(f\"{counter}. {stripped}\")\n",
    "        counter += 1\n",
    "    print(\"üìù Generated Questions:\\n\")\n",
    "    print(\"\\n\".join(cleaned))\n",
    "    if result.total_tokens is not None:\n",
    "        print(f\"\\n(tokens: {result.total_tokens}, retries: {result.retries})\")\n",
    "\n",
    "# (Optional) A quick function to generate n factual-only questions (could be reused elsewhere)\n",
    "\n",
    "\n",
    "def generate_factual_only(passage: str, n: int = 3) -> list[str]:\n",
    "    p = f\"Generate {n} distinct factual recall questions (no answers) from the passage. Number them. Passage: {passage}\\nQuestions:\"  # noqa: E501\n",
    "    r = get_completion(p, max_tokens=120, temperature=0.25)\n",
    "    if not r or not r.text:\n",
    "        return []\n",
    "    return [ln.split('.', 1)[1].strip() if '.' in ln else ln.strip() for ln in r.text.splitlines() if ln.strip()]\n",
    "\n",
    "\n",
    "# Example usage of helper (not required for main output)\n",
    "extra = generate_factual_only(source_passage, 2)\n",
    "if extra:\n",
    "    print(\"\\nüîé Extra Factual Questions:\")\n",
    "    for q in extra:\n",
    "        print(f\"- {q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f37ea55",
   "metadata": {},
   "source": [
    "## ‚úÖ Use Case 12: Entity Extraction with Explanation\n",
    "\n",
    "**Input:**\n",
    "\"Apple announced a new partnership with Tesla to develop AI-powered batteries in California.\"\n",
    "\n",
    "**Sample Output:**\n",
    "\n",
    "* **Apple** ‚Äì Technology company forming a partnership.\n",
    "* **Tesla** ‚Äì Automotive/energy company collaborating on batteries.\n",
    "* **California** ‚Äì Location where the initiative is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a080195c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Use Case 12: Entity Extraction with Explanation\n",
    "\"\"\"Extract named entities with short functional explanations.\n",
    "Demonstrates:\n",
    "- Instruction for schema-like output\n",
    "- Deterministic extraction (low temperature)\n",
    "- Light post-processing to normalize formatting\n",
    "\"\"\"\n",
    "\n",
    "text = (\n",
    "    \"Apple announced a new partnership with Tesla to develop AI-powered batteries in California. \"\n",
    "    \"The collaboration will focus on scalable sustainable energy storage, leveraging Apple's design \"\n",
    "    \"ecosystem and Tesla's manufacturing expertise.\"\n",
    ")\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You are an information extraction assistant.\n",
    "Task: Identify distinct real-world entities in the passage and list each with:\n",
    "- Canonical surface form\n",
    "- Type (choose from: Organization, Location, Technology, Initiative)\n",
    "- One concise explanation (‚â§ 15 words) describing its role in context.\n",
    "\n",
    "Rules:\n",
    "- Do not invent entities not present.\n",
    "- Do not repeat the same entity.\n",
    "- If something is both a project and a company, choose the most salient type for this context.\n",
    "- Avoid marketing language; be factual.\n",
    "\n",
    "Return ONLY a Markdown bullet list in the form:\n",
    "* Entity ‚Äì Type: Explanation\n",
    "\n",
    "Passage:\n",
    "{text}\n",
    "\n",
    "List:\n",
    "\"\"\".strip()\n",
    "\n",
    "result = get_completion(prompt, max_tokens=220, temperature=0.15)\n",
    "\n",
    "if not result or not result.text:\n",
    "    print(\"‚ùå No response received.\")\n",
    "else:\n",
    "    raw = result.text.strip()\n",
    "    lines = [ln.strip() for ln in raw.splitlines() if ln.strip()]\n",
    "    # Normalize: ensure bullets start with '* '\n",
    "    normalized = []\n",
    "    for ln in lines:\n",
    "        if not ln.startswith(('*', '-')):\n",
    "            ln = '* ' + ln.lstrip('*- ')  # force bullet\n",
    "        else:\n",
    "            # replace leading '-' with '*'\n",
    "            ln = '* ' + ln.lstrip('*- ').strip()\n",
    "        normalized.append(ln)\n",
    "    print(\"üßæ Extracted Entities:\\n\")\n",
    "    print(\"\\n\".join(normalized))\n",
    "    if result.total_tokens is not None:\n",
    "        print(f\"\\n(tokens: {result.total_tokens}, retries: {result.retries})\")\n",
    "\n",
    "# Optional helper for reuse\n",
    "\n",
    "\n",
    "def extract_entities(passage: str) -> list[tuple[str, str, str]]:\n",
    "    \"\"\"Return list of (entity, type, explanation).\"\"\"\n",
    "    p = f\"Identify entities with type and ‚â§10-word explanation. Passage: {passage}\\nList:\"  # noqa: E501\n",
    "    r = get_completion(p, max_tokens=140, temperature=0.2)\n",
    "    if not r or not r.text:\n",
    "        return []\n",
    "    triples = []\n",
    "    import re\n",
    "    for line in r.text.splitlines():\n",
    "        line = line.strip('*- ').strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        # Pattern: Entity ‚Äì Type: Explanation\n",
    "        m = re.match(r\"(.+?)\\s+[‚Äì-]\\s+(\\w+):\\s+(.+)\", line)\n",
    "        if m:\n",
    "            triples.append((m.group(1).strip(), m.group(\n",
    "                2).strip(), m.group(3).strip()))\n",
    "    return triples\n",
    "\n",
    "\n",
    "# Quick demonstration (not required)\n",
    "_demo = extract_entities(text)\n",
    "if _demo:\n",
    "    print(\"\\nüîç Parsed Triples:\")\n",
    "    for ent, typ, expl in _demo:\n",
    "        print(f\"- {ent} ({typ}): {expl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2395e279",
   "metadata": {},
   "source": [
    "## ‚úÖ Use Case 13: Paraphrasing / Rewriting\n",
    "\n",
    "**Input:**\n",
    "\"The proliferation of interconnected devices has exponentially increased the complexity of ensuring cybersecurity across distributed networks.\"\n",
    "\n",
    "**Sample Output:**\n",
    "\"With more connected devices, keeping networks secure has become much harder.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc72b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Use Case 13: Paraphrasing / Rewriting\n",
    "\"\"\"Generate multiple paraphrase variants with different constraints.\n",
    "Demonstrates:\n",
    "- Style-controlled rewriting\n",
    "- Brevity vs. fidelity trade-offs\n",
    "- Enforcing no meaning drift\n",
    "\"\"\"\n",
    "\n",
    "original_sentence = (\n",
    "    \"The proliferation of interconnected devices has exponentially increased the complexity of ensuring \"\n",
    "    \"cybersecurity across distributed networks.\"\n",
    ")\n",
    "\n",
    "rewrite_specs = [\n",
    "    {\n",
    "        \"label\": \"Plain Concise\",\n",
    "        \"instruction\": \"Rewrite in everyday language, ‚â§ 22 words, preserve meaning.\",\n",
    "        \"temperature\": 0.35,\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Executive Summary\",\n",
    "        \"instruction\": \"Rewrite as a crisp executive summary (‚â§ 18 words) highlighting the core risk.\",\n",
    "        \"temperature\": 0.4,\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Technical Precision\",\n",
    "        \"instruction\": \"Rewrite with precise technical wording, neutral tone, no adjectives, ‚â§ 28 words.\",\n",
    "        \"temperature\": 0.3,\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Minimal Compression\",\n",
    "        \"instruction\": \"Compress to ‚â§ 12 words retaining key idea.\",\n",
    "        \"temperature\": 0.45,\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"üßæ Original:\\n\" + original_sentence + \"\\n\")\n",
    "\n",
    "for spec in rewrite_specs:\n",
    "    prompt = f\"\"\"\n",
    "You are a rewriting assistant.\n",
    "Task: {spec['instruction']}\n",
    "Rules:\n",
    "- No new facts.\n",
    "- Do not remove the notion of increased complexity or distributed networks.\n",
    "- Output ONLY the rewritten sentence (no label, no quotes).\n",
    "\n",
    "Original: {original_sentence}\n",
    "\n",
    "Rewritten:\n",
    "\"\"\".strip()\n",
    "    res = get_completion(prompt, max_tokens=70,\n",
    "                         temperature=spec['temperature'])\n",
    "    if res and res.text:\n",
    "        print(f\"‚úèÔ∏è {spec['label']}: {res.text.strip()}\\n\")\n",
    "    else:\n",
    "        print(f\"‚ùå {spec['label']}: (no response)\\n\")\n",
    "\n",
    "# Optional: Quick single paraphrase helper\n",
    "\n",
    "\n",
    "def paraphrase(text: str, style: str = \"neutral concise\") -> str:\n",
    "    p = f\"Rewrite in a {style} style without changing meaning. Text: {text}\\nRewritten:\"  # noqa: E501\n",
    "    r = get_completion(p, max_tokens=60, temperature=0.4)\n",
    "    return r.text.strip() if r and r.text else \"(no result)\"\n",
    "\n",
    "\n",
    "print(\"--- Helper Demo ---\")\n",
    "print(paraphrase(original_sentence, \"succinct plain English\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae01be8",
   "metadata": {},
   "source": [
    "\n",
    "## ‚úÖ Use Case 14: Email / Document Drafting\n",
    "\n",
    "**Input:**\n",
    "\"Announce a team meeting for Friday at 3 PM to discuss the quarterly results.\"\n",
    "\n",
    "**Sample Output:**\n",
    "\"Subject: Team Meeting ‚Äì Friday at 3 PM\n",
    "Hi Team,\n",
    "We will have a meeting on Friday at 3 PM to review our quarterly results. Please make sure to attend.\n",
    "Best regards,\n",
    "\\[Your Name]\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03f75eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Use Case 14: Email / Document Drafting\n",
    "\"\"\"Draft a professional email from a concise instruction.\n",
    "Demonstrates:\n",
    "- Structured multi-section output\n",
    "- Tone control and placeholders\n",
    "- Optional variant generation (formal vs. friendly)\n",
    "\"\"\"\n",
    "\n",
    "instruction = \"Announce a team meeting for Friday at 3 PM to discuss the quarterly results.\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You are an assistant that drafts clear professional internal emails.\n",
    "Instruction: {instruction}\n",
    "\n",
    "Requirements:\n",
    "- Include a subject line (prefix with 'Subject:').\n",
    "- Opening greeting addressing 'Team'.\n",
    "- Body: purpose, meeting details (date, time, topic), expectation to attend, optional prep mention.\n",
    "- Closing with a neutral professional sign-off (e.g., 'Best regards,').\n",
    "- Insert placeholder '[Your Name]' for sender.\n",
    "- Keep to ‚â§ 140 words.\n",
    "- No exaggerated enthusiasm or emojis.\n",
    "\n",
    "Return ONLY the email text. No markdown fences.\n",
    "\n",
    "Email:\n",
    "\"\"\".strip()\n",
    "\n",
    "result = get_completion(prompt, max_tokens=260, temperature=0.35)\n",
    "\n",
    "if not result or not result.text:\n",
    "    print(\"‚ùå No response received.\")\n",
    "else:\n",
    "    email = result.text.strip()\n",
    "    print(\"üìß Draft Email:\\n\")\n",
    "    print(email)\n",
    "    if result.total_tokens is not None:\n",
    "        print(f\"\\n(tokens: {result.total_tokens}, retries: {result.retries})\")\n",
    "\n",
    "# Optional: Generate a friendlier variant\n",
    "variant_prompt = f\"Rewrite the following email in a slightly friendlier but still professional tone. No added details.\\nEmail:\\n{email}\\n\\nRewritten:\" if result and result.text else None\n",
    "if variant_prompt:\n",
    "    variant_res = get_completion(\n",
    "        variant_prompt, max_tokens=240, temperature=0.55)\n",
    "    if variant_res and variant_res.text:\n",
    "        print(\"\\nü§ù Friendly Variant:\\n\")\n",
    "        print(variant_res.text.strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

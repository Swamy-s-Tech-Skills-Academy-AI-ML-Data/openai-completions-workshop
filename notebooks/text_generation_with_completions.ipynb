{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "379fa59e",
   "metadata": {},
   "source": [
    "# üöÄ Text Generation Apps using Python (Completions Only)\n",
    "\n",
    "This notebook showcases **14 text generation use cases** using OpenAI **Completions API**.  \n",
    "Each section includes a description, the Python code, and expected output.\n",
    "\n",
    "---\n",
    "\n",
    "## üìë Table of Contents\n",
    "1. [Summarization](#-Use-Case-1-Summarization)  \n",
    "2. [Sentiment Classification](#-Use-Case-2-Sentiment-Classification)  \n",
    "3. [Multilingual Generation + Translation](#-Use-Case-3-Multilingual-Generation--Translation)  \n",
    "4. [Semantic Interpretation of Idiom](#-Use-Case-4-Semantic-Interpretation-of-Idiom)  \n",
    "5. [Factual Recall / Explanatory Response](#-Use-Case-5-Factual-Recall--Explanatory-Response)  \n",
    "6. [Code Generation & Explanation](#-Use-Case-6-Code-Generation--Explanation)  \n",
    "7. [Conversational Agent (FAQ Assistant)](#-Use-Case-7-Conversational-Agent-FAQ-Assistant)  \n",
    "8. [Style Transfer / Tone Adaptation](#-Use-Case-8-Style-Transfer--Tone-Adaptation)  \n",
    "9. [Data-to-Text Generation](#-Use-Case-9-Data-to-Text-Generation)  \n",
    "10. [Creative Writing](#-Use-Case-10-Creative-Writing)  \n",
    "11. [Question Generation](#-Use-Case-11-Question-Generation)  \n",
    "12. [Entity Extraction with Explanation](#-Use-Case-12-Entity-Extraction-with-Explanation)  \n",
    "13. [Paraphrasing / Rewriting](#-Use-Case-13-Paraphrasing--Rewriting)  \n",
    "14. [Email / Document Drafting](#-Use-Case-14-Email--Document-Drafting)  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cfbf4a",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "- Ensure your `.env` file contains `OPENAI_API_KEY` (never hardcode secrets).\n",
    "- Use `python-dotenv` to load environment variables securely.\n",
    "- Import and initialize the `OpenAI` client with your API key.\n",
    "- Define a reusable helper function `get_completion(prompt, ...)` to call the OpenAI Completions API with configurable model, temperature, and max tokens.\n",
    "- Prefer environment variables for model selection and temperature overrides.\n",
    "- Example setup and usage are shown in the next code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1034991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (skip if already installed)\n",
    "# %pip install openai tiktoken python-dotenv\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Sequence\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI, APIError, RateLimitError\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Logging (avoid reconfiguring if handlers already exist)\n",
    "# -------------------------------------------------------------------\n",
    "if not logging.getLogger().handlers:\n",
    "    logging.basicConfig(level=logging.INFO,\n",
    "                        format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Environment loading & helpers\n",
    "# -------------------------------------------------------------------\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY is not set in environment or .env file\")\n",
    "\n",
    "DEFAULT_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-3.5-turbo-instruct\")\n",
    "\n",
    "\n",
    "def _env_float(name: str, default: float) -> float:\n",
    "    raw = os.getenv(name)\n",
    "    if not raw:\n",
    "        return default\n",
    "    # Ignore Windows TEMP style paths accidentally picked up\n",
    "    if os.path.sep in raw:\n",
    "        return default\n",
    "    try:\n",
    "        val = float(raw)\n",
    "        if 0 <= val <= 1:\n",
    "            return val\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return default\n",
    "\n",
    "\n",
    "def _env_int(name: str, default: int, min_val: int = 1, max_val: int = 4000) -> int:\n",
    "    raw = os.getenv(name)\n",
    "    if not raw:\n",
    "        return default\n",
    "    try:\n",
    "        val = int(raw)\n",
    "        if min_val <= val <= max_val:\n",
    "            return val\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return default\n",
    "\n",
    "\n",
    "def _get_client() -> OpenAI:\n",
    "    # Could be extended for per-request customization later\n",
    "    return OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "\n",
    "client = _get_client()\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Result container\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CompletionResult:\n",
    "    text: str\n",
    "    model: str\n",
    "    prompt_tokens: Optional[int] = None\n",
    "    completion_tokens: Optional[int] = None\n",
    "    total_tokens: Optional[int] = None\n",
    "    retries: int = 0\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Core helper with light retries\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def get_completion(\n",
    "    prompt: str,\n",
    "    model: str = DEFAULT_MODEL,\n",
    "    max_tokens: Optional[int] = None,\n",
    "    temperature: Optional[float] = None,\n",
    "    stop: Optional[Sequence[str]] = None,\n",
    "    log_response: bool = False,\n",
    "    max_retries: int = 3,\n",
    "    backoff_base: float = 1.5,\n",
    ") -> Optional[CompletionResult]:\n",
    "    \"\"\"Call OpenAI Completions API with resilience & env overrides.\n",
    "\n",
    "    Environment overrides:\n",
    "      OPENAI_MODEL          -> default model if not passed\n",
    "      OAI_TEMP / OAI_TEMPERATURE -> temperature (0..1)\n",
    "      OAI_MAX_TOKENS        -> max_tokens upper bound\n",
    "    \"\"\"\n",
    "    if not prompt or not prompt.strip():\n",
    "        raise ValueError(\"Prompt must be a non-empty string\")\n",
    "\n",
    "    # Apply env fallbacks only if caller did not specify\n",
    "    if temperature is None:\n",
    "        temperature = _env_float(\n",
    "            \"OAI_TEMP\", _env_float(\"OAI_TEMPERATURE\", 0.7))\n",
    "    if max_tokens is None:\n",
    "        max_tokens = _env_int(\"OAI_MAX_TOKENS\", 200, 1, 4096)\n",
    "\n",
    "    attempt = 0\n",
    "    last_error: Optional[Exception] = None\n",
    "    while attempt <= max_retries:\n",
    "        try:\n",
    "            start = time.time()\n",
    "            resp = client.completions.create(\n",
    "                model=model,\n",
    "                prompt=prompt.strip(),\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "                stop=list(stop) if stop else None,\n",
    "            )\n",
    "            latency = (time.time() - start) * 1000\n",
    "            choice = resp.choices[0]\n",
    "            usage = getattr(resp, \"usage\", None)\n",
    "            if log_response:\n",
    "                logger.debug(\"Raw response: %s\", resp)\n",
    "            logger.info(\n",
    "                \"completion model=%s tokens=%s latency=%.1fms retries=%d\",  # noqa: E501\n",
    "                model,\n",
    "                getattr(usage, \"total_tokens\", \"?\"),\n",
    "                latency,\n",
    "                attempt,\n",
    "            )\n",
    "            return CompletionResult(\n",
    "                text=choice.text.strip(),\n",
    "                model=model,\n",
    "                prompt_tokens=getattr(usage, \"prompt_tokens\", None),\n",
    "                completion_tokens=getattr(usage, \"completion_tokens\", None),\n",
    "                total_tokens=getattr(usage, \"total_tokens\", None),\n",
    "                retries=attempt,\n",
    "            )\n",
    "        except (RateLimitError, APIError) as e:\n",
    "            last_error = e\n",
    "            if attempt == max_retries:\n",
    "                logger.error(\"Max retries reached (%d): %s\", max_retries, e)\n",
    "                break\n",
    "            sleep_for = backoff_base ** attempt + random.uniform(0, 0.25)\n",
    "            logger.warning(\"Transient error (%s). Retrying in %.2fs (attempt %d/%d)\",\n",
    "                           e.__class__.__name__, sleep_for, attempt + 1, max_retries)\n",
    "            time.sleep(sleep_for)\n",
    "            attempt += 1\n",
    "            continue\n",
    "        except Exception as e:  # Non-retryable\n",
    "            logger.error(\"Completion failed: %s\", e)\n",
    "            last_error = e\n",
    "            break\n",
    "    return None\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Example usage (kept minimal for notebook demonstration)\n",
    "# -------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    test_prompt = \"Write a haiku about AI and the future.\"\n",
    "    result = get_completion(test_prompt, log_response=True)\n",
    "    if result:\n",
    "        print(\"‚úÖ Completion result:\\n\", result.text)\n",
    "        if result.total_tokens is not None:\n",
    "            print(\n",
    "                f\"(tokens: {result.total_tokens}, retries: {result.retries})\")\n",
    "    else:\n",
    "        print(\"‚ùå No response received.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221d9864",
   "metadata": {},
   "source": [
    "## ‚úÖ Use Case 1: Summarization\n",
    "**Input:**  \n",
    "Sparse vector feature indexing allows scalable search over events by turning categorical attributes into an extremely wide, mostly empty vector...\n",
    "\n",
    "**Sample Output:**  \n",
    "\"It converts categorical features into sparse vectors for scalable search. The trade-off is efficiency vs. memory/compute cost at high cardinality.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2904a9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Use Case 1: Summarization Code\n",
    "\n",
    "technical_paragraph = \"\"\"\n",
    "Sparse vector feature indexing allows scalable search over events by turning \n",
    "categorical attributes into an extremely wide, mostly empty vector. Each distinct \n",
    "token (e.g. event_type:click, browser:mobile) maps to a coordinate set to 1 while \n",
    "all others remain 0. This removes unintended ordering between categories and keeps \n",
    "distance metrics meaningful, but at very high cardinality memory and compute \n",
    "overhead grow rapidly, motivating hashing tricks or learned embeddings.\n",
    "\"\"\".strip()\n",
    "\n",
    "summary_prompt = f\"Summarize the following paragraph in 2 crisp sentences focusing on purpose and trade-offs:\\n\\n{technical_paragraph}\\n\\nSummary:\"\n",
    "\n",
    "# Call completions API using helper\n",
    "summary = get_completion(\n",
    "    summary_prompt, max_tokens=120, temperature=0.4)\n",
    "\n",
    "print(\"‚úÖ Summary:\", summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c7a222",
   "metadata": {},
   "source": [
    "## ‚úÖ Use Case 2: Sentiment Classification\n",
    "**Input:**  \n",
    "\"The user interface feels sluggish, but the reporting features are fantastic.\"\n",
    "\n",
    "**Sample Output:**  \n",
    "\"Neutral\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e3576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Use Case 2: Sentiment Classification (Multiple Inputs)\n",
    "\n",
    "# Sample inputs\n",
    "input_texts = [\n",
    "    \"The user interface feels sluggish, but the reporting features are fantastic.\",  # Neutral\n",
    "    \"I love the new dashboard design, it‚Äôs clean and very intuitive.\",               # Positive\n",
    "    \"The system keeps crashing frequently, making it hard to use.\"                   # Negative\n",
    "]\n",
    "\n",
    "# Loop through each input and classify sentiment\n",
    "for input_text in input_texts:\n",
    "    sentiment_prompt = f\"\"\"\n",
    "    Classify the sentiment of the following text strictly as Positive, Negative, or Neutral.\n",
    "    If the text contains both positive and negative aspects, classify it as Neutral.\n",
    "\n",
    "    Text: \"{input_text}\"\n",
    "\n",
    "    Sentiment:\n",
    "    \"\"\"\n",
    "\n",
    "    # Call completions API using helper (returns CompletionResult or None)\n",
    "    result = get_completion(\n",
    "        sentiment_prompt,\n",
    "        max_tokens=20,\n",
    "        temperature=0  # deterministic output\n",
    "    )\n",
    "\n",
    "    if result and result.text:\n",
    "        sentiment_text = result.text.strip()\n",
    "    else:\n",
    "        sentiment_text = \"(no response)\"\n",
    "\n",
    "    print(f\"‚úÖ Text: {input_text}\")\n",
    "    print(f\"‚úÖ Sentiment: {sentiment_text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c6fc11",
   "metadata": {},
   "source": [
    "## ‚úÖ Use Case 3: Multilingual Generation + Translation\n",
    "**Input:**  \n",
    "\"Artificial Intelligence will transform education.\"\n",
    "\n",
    "**Sample Output:**  \n",
    "- French: \"L'intelligence artificielle transformera l'√©ducation.\"  \n",
    "- Spanish: \"La inteligencia artificial transformar√° la educaci√≥n.\"  \n",
    "- Hindi: \"‡§ï‡•É‡§§‡•ç‡§∞‡§ø‡§Æ ‡§¨‡•Å‡§¶‡•ç‡§ß‡§ø‡§Æ‡§§‡•ç‡§§‡§æ ‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ ‡§ï‡•ã ‡§¨‡§¶‡§≤ ‡§¶‡•á‡§ó‡•Ä‡•§\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56b1e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Use Case 3: Multilingual Generation + Translation\n",
    "\n",
    "input_text = \"Artificial Intelligence will transform education.\"\n",
    "languages = [\"French\", \"Spanish\", \"Hindi\"]\n",
    "\n",
    "for lang in languages:\n",
    "    translation_prompt = f\"Translate the following text into {lang}:\\n\\n{input_text}\"\n",
    "    translation = get_completion(\n",
    "        translation_prompt, max_tokens=60, temperature=0.3)\n",
    "    print(f\"‚úÖ {lang}: {translation}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1177ce",
   "metadata": {},
   "source": [
    "## ‚úÖ Use Case 4: Semantic Interpretation of Idiom\n",
    "**Input:**  \n",
    "\"Kick the bucket\"\n",
    "\n",
    "**Sample Output:**  \n",
    "\"It means someone has died.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae68ab33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Use Case 4: Semantic Interpretation of Idiom\n",
    "\n",
    "idioms = [\n",
    "    \"kick the bucket\",\n",
    "    \"spill the beans\",\n",
    "    \"hit the sack\",\n",
    "    \"once in a blue moon\",\n",
    "    \"break the ice\",\n",
    "]\n",
    "\n",
    "for idiom in idioms:\n",
    "    prompt = f\"\"\"\n",
    "Explain the meaning of the idiom below in one concise, literal-friendly plain English sentence.\n",
    "Avoid extra commentary or cultural notes unless essential.\n",
    "Return ONLY the meaning (no leading labels).\n",
    "\n",
    "Idiom: \"{idiom}\"\n",
    "\n",
    "Meaning:\n",
    "\"\"\".strip()\n",
    "\n",
    "    meaning = get_completion(\n",
    "        prompt,\n",
    "        model=\"gpt-3.5-turbo-instruct\",\n",
    "        max_tokens=60,\n",
    "        temperature=0  # deterministic for stable definitions\n",
    "    )\n",
    "\n",
    "    if meaning:\n",
    "        print(f\"üó£Ô∏è {idiom} -> {meaning}\\n\\n\")\n",
    "    else:\n",
    "        print(f\"‚ùå No response for idiom: {idiom}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450e8e44",
   "metadata": {},
   "source": [
    "## ‚úÖ Use Case 5: Factual Recall / Explanatory Response\n",
    "**Input:**  \n",
    "\"Why is the sky blue?\"\n",
    "\n",
    "**Sample Output:**  \n",
    "\"The sky looks blue because sunlight is scattered by air molecules. Blue light is scattered more strongly than other colors, so we mostly see blue.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6538f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Use Case 5: Factual Recall / Explanatory Response\n",
    "\n",
    "question = \"Why is the sky blue?\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Provide a concise scientific explanation (2 sentences) for the question below.\n",
    "Avoid fluff; focus on the underlying physical phenomenon.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "result = get_completion(prompt, max_tokens=120, temperature=0.2)\n",
    "\n",
    "if result and result.text:\n",
    "    print(\"‚ùì Question:\", question)\n",
    "    print(\"üß™ Answer:\", result.text.strip())\n",
    "    if result.total_tokens is not None:\n",
    "        print(f\"(tokens: {result.total_tokens}, retries: {result.retries})\")\n",
    "else:\n",
    "    print(\"‚ùå No response received.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3663aa09",
   "metadata": {},
   "source": [
    "## ‚úÖ Use Case 6: Code Generation & Explanation\n",
    "**Input:**  \n",
    "\"Write a Python function that checks if a number is prime.\"\n",
    "\n",
    "**Sample Output:**  \n",
    "```python\n",
    "def is_prime(n):\n",
    "    if n < 2:\n",
    "        return False\n",
    "    for i in range(2, int(n**0.5) + 1):\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "    return True\n",
    "````\n",
    "\n",
    "**Explanation:**\n",
    "\"The function checks divisibility from 2 up to ‚àön. If any divisor is found, it returns False; otherwise, True.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e8cd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Use Case 6: Code Generation & Explanation\n",
    "\n",
    "problem = \"Write a Python function that checks if a number is prime.\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You are a senior Python engineer.\n",
    "Generate clean, production-quality code that fulfills the requirement below.\n",
    "After the code, add a single concise explanation sentence.\n",
    "Use exactly this delimiter line before the explanation:\n",
    "---EXPLANATION---\n",
    "\n",
    "Requirement:\n",
    "{problem}\n",
    "\n",
    "Return ONLY:\n",
    "1. A single Python code block\n",
    "2. The delimiter line\n",
    "3. One concise explanation sentence\n",
    "\"\"\".strip()\n",
    "\n",
    "result = get_completion(prompt, max_tokens=220, temperature=0)\n",
    "\n",
    "if not result or not result.text:\n",
    "    print(\"‚ùå No response received.\")\n",
    "else:\n",
    "    raw = result.text.strip()\n",
    "    # Attempt to split code and explanation\n",
    "    parts = raw.split(\"---EXPLANATION---\", 1)\n",
    "    code_part = raw\n",
    "    explanation = None\n",
    "    if len(parts) == 2:\n",
    "        code_part, explanation = parts[0].strip(), parts[1].strip()\n",
    "\n",
    "    print(\"üß© Generated Code:\")\n",
    "    print(code_part)\n",
    "    if explanation:\n",
    "        print(\"\\nüìù Explanation:\", explanation)\n",
    "    if result.total_tokens is not None:\n",
    "        print(f\"\\n(tokens: {result.total_tokens}, retries: {result.retries})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e6919f",
   "metadata": {},
   "source": [
    "## ‚úÖ Use Case 7: Conversational Agent (FAQ Assistant)\n",
    "\n",
    "**Input:**\n",
    "\"What is the purpose of Git?\"\n",
    "\n",
    "**Sample Output:**\n",
    "\"Git is a version control system that tracks changes in code, helps collaboration, and manages project history.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0f6c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Use Case 7: Conversational Agent (FAQ Assistant)\n",
    "\n",
    "# A tiny in-memory FAQ knowledge base (could be loaded from file/db in real apps)\n",
    "faq_kb = {\n",
    "    \"What is Git?\": \"Git is a distributed version control system for tracking changes in source code.\",\n",
    "    \"Why use version control?\": \"It enables history, collaboration, branching, and safe experimentation.\",\n",
    "    \"What is a commit?\": \"A commit records a snapshot of your staged changes with a message.\",\n",
    "    \"What is branching?\": \"Branching lets you diverge from the main line of development to work independently.\",\n",
    "    \"What is a pull request?\": \"A pull request proposes merging changes from one branch into another, supporting review.\",\n",
    "}\n",
    "\n",
    "# Simulated user questions (some exact, some paraphrased / partially overlapping)\n",
    "user_queries = [\n",
    "    \"What is the purpose of Git?\",\n",
    "    \"Explain branching in simple terms.\",\n",
    "    \"Why should teams adopt version control?\",\n",
    "]\n",
    "\n",
    "system_instruction = (\n",
    "    \"You are a concise technical FAQ assistant. Answer ONLY from the provided FAQ entries. \"\n",
    "    \"If the answer is not present, reply with 'I don't have that information.' Keep answers to one or two sentences.\"\n",
    ")\n",
    "\n",
    "history: list[tuple[str, str]] = []  # (user_question, answer)\n",
    "\n",
    "for q in user_queries:\n",
    "    # Build a retrieval-aware prompt by listing KB and prior turns.\n",
    "    kb_block = \"\\n\".join(f\"Q: {k}\\nA: {v}\" for k, v in faq_kb.items())\n",
    "    convo_block = \"\\n\".join(\n",
    "        # last few turns\n",
    "        f\"User: {hq}\\nAssistant: {ha}\" for hq, ha in history[-4:])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "{system_instruction}\n",
    "\n",
    "FAQ Knowledge Base:\n",
    "{kb_block}\n",
    "\n",
    "Conversation So Far:\n",
    "{convo_block if convo_block else '(no previous turns)'}\n",
    "\n",
    "User Question: {q}\n",
    "\n",
    "Answer (draw ONLY from KB or say you don't have it):\n",
    "\"\"\".strip()\n",
    "\n",
    "    result = get_completion(prompt, max_tokens=110, temperature=0.2)\n",
    "\n",
    "    if result and result.text:\n",
    "        answer = result.text.strip()\n",
    "    else:\n",
    "        answer = \"(no response)\"\n",
    "\n",
    "    history.append((q, answer))\n",
    "    print(f\"üôã User: {q}\")\n",
    "    print(f\"ü§ñ Assistant: {answer}\\n\")\n",
    "\n",
    "# Show a compact transcript summary\n",
    "print(\"--- Transcript Summary ---\")\n",
    "for uq, ans in history:\n",
    "    print(f\"Q: {uq}\\nA: {ans}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7da3675",
   "metadata": {},
   "source": [
    "\n",
    "## ‚úÖ Use Case 8: Style Transfer / Tone Adaptation\n",
    "\n",
    "**Input:**\n",
    "\"Please confirm receipt of this document at the earliest.\"\n",
    "\n",
    "**Sample Output:**\n",
    "\"Hey, just let me know when you get this doc!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab64e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ‚úÖ Use Case 8: Style Transfer / Tone Adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb220da",
   "metadata": {},
   "source": [
    "## ‚úÖ Use Case 9: Data-to-Text Generation\n",
    "\n",
    "**Input:**\n",
    "Region: North\n",
    "Sales: \\$50,000\n",
    "Increase: 10%\n",
    "\n",
    "**Sample Output:**\n",
    "\"The North region generated \\$50,000 in sales, reflecting a 10% increase compared to the previous period.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f58db56",
   "metadata": {},
   "source": [
    "## ‚úÖ Use Case 10: Creative Writing\n",
    "\n",
    "**Input:**\n",
    "\"A robot learning to paint\"\n",
    "\n",
    "**Sample Output:**\n",
    "\"The robot dipped its brush in bright colors, each stroke a clumsy attempt at beauty. Slowly, it learned to create sunsets and flowers. Its creators realized the machine had found its soul in art.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd24c6a",
   "metadata": {},
   "source": [
    "## ‚úÖ Use Case 11: Question Generation\n",
    "\n",
    "**Input:**\n",
    "\"The Pacific Ocean is the largest and deepest of Earth's oceanic divisions. It extends from the Arctic Ocean in the north to the Southern Ocean in the south.\"\n",
    "\n",
    "**Sample Output:**\n",
    "\n",
    "1. What is the largest ocean on Earth?\n",
    "2. How deep is the Pacific Ocean compared to other oceans?\n",
    "3. Which oceans border the Pacific to the north and south?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f37ea55",
   "metadata": {},
   "source": [
    "## ‚úÖ Use Case 12: Entity Extraction with Explanation\n",
    "\n",
    "**Input:**\n",
    "\"Apple announced a new partnership with Tesla to develop AI-powered batteries in California.\"\n",
    "\n",
    "**Sample Output:**\n",
    "\n",
    "* **Apple** ‚Äì Technology company forming a partnership.\n",
    "* **Tesla** ‚Äì Automotive/energy company collaborating on batteries.\n",
    "* **California** ‚Äì Location where the initiative is happening."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2395e279",
   "metadata": {},
   "source": [
    "## ‚úÖ Use Case 13: Paraphrasing / Rewriting\n",
    "\n",
    "**Input:**\n",
    "\"The proliferation of interconnected devices has exponentially increased the complexity of ensuring cybersecurity across distributed networks.\"\n",
    "\n",
    "**Sample Output:**\n",
    "\"With more connected devices, keeping networks secure has become much harder.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae01be8",
   "metadata": {},
   "source": [
    "\n",
    "## ‚úÖ Use Case 14: Email / Document Drafting\n",
    "\n",
    "**Input:**\n",
    "\"Announce a team meeting for Friday at 3 PM to discuss the quarterly results.\"\n",
    "\n",
    "**Sample Output:**\n",
    "\"Subject: Team Meeting ‚Äì Friday at 3 PM\n",
    "Hi Team,\n",
    "We will have a meeting on Friday at 3 PM to review our quarterly results. Please make sure to attend.\n",
    "Best regards,\n",
    "\\[Your Name]\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9514ba30",
   "metadata": {},
   "source": [
    "# OpenAI Completions Workshop Notebook\n",
    "\n",
    "End-to-end exploration of OpenAI *text completion* patterns: listing models, baseline prompting, refinement, multiple candidates, token awareness, penalties, and streaming.\n",
    "\n",
    "Environment requirement: `OPENAI_API_KEY` must be set (or a `.env` loaded earlier). Optional: set `OPENAI_MODEL` to override the default model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "975bc434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "# Imports & client initialization\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "from openai import OpenAI\n",
    "\n",
    "API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "if not API_KEY:\n",
    "    raise EnvironmentError(\n",
    "        'OPENAI_API_KEY not set. Set it in your environment or .env file.')\n",
    "\n",
    "# Override by exporting OPENAI_MODEL if desired\n",
    "MODEL = os.getenv('OPENAI_MODEL', 'gpt-4o-mini')\n",
    "client = OpenAI(api_key=API_KEY)\n",
    "print(f'Using model: {MODEL}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090f098f",
   "metadata": {},
   "source": [
    "## 1. List Available Models\n",
    "Shows the raw list returned by the Models API (truncated for brevity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85adbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = client.models.list()\n",
    "print(f'Total models: {len(models.data)}')\n",
    "for m in models.data[:10]:  # show first 10\n",
    "    print(f'{m.id:40} | owned_by={getattr(m, 'owned_by', '?')}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a09f93",
   "metadata": {},
   "source": [
    "## 2. Baseline Completion\n",
    "Single prompt -> single completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6967cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    'Suggest three concise brand name ideas for an eco-friendly smart home cleaning device. '\n",
    "    'Each name should feel modern, memorable, and avoid generic buzzwords.'\n",
    ")\n",
    "response = client.completions.create(\n",
    "    model=MODEL, prompt=prompt, max_tokens=120, temperature=0.7)\n",
    "print(response.choices[0].text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4570efa4",
   "metadata": {},
   "source": [
    "## 3. Multiple Candidates & Temperature Sweep\n",
    "Generate several candidates to compare diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396ca8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "temps = [0.2, 0.7, 1.0]\n",
    "for t in temps:\n",
    "    resp = client.completions.create(\n",
    "        model=MODEL, prompt=prompt, max_tokens=100, n=3, temperature=t)\n",
    "    print(f'--- temperature={t} ---')\n",
    "    for i, choice in enumerate(resp.choices, 1):\n",
    "        print(f'[{i}] {choice.text.strip()}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ad6694",
   "metadata": {},
   "source": [
    "## 4. Presence & Frequency Penalties\n",
    "Encourage novelty or reduce repetition. (Values here are illustrative; tune per task.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71779bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for presence in [0.0, 0.8]:\n",
    "    for freq in [0.0, 0.8]:\n",
    "        resp = client.completions.create(\n",
    "            model=MODEL, prompt=prompt, max_tokens=80, temperature=0.7,\n",
    "            presence_penalty=presence, frequency_penalty=freq, n=1)\n",
    "        print(f'presence_penalty={presence} frequency_penalty={freq}')\n",
    "        print(resp.choices[0].text.strip(), '---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f87eb9",
   "metadata": {},
   "source": [
    "## 5. Token Counting (tiktoken)\n",
    "Estimate prompt token usage for budgeting / compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c9c4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import tiktoken\n",
    "    enc = tiktoken.get_encoding('cl100k_base')\n",
    "    token_count = len(enc.encode(prompt))\n",
    "    print('Prompt tokens:', token_count)\n",
    "except Exception as e:\n",
    "    print('tiktoken not installed or error occurred:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaee846",
   "metadata": {},
   "source": [
    "## 6. Streaming Responses\n",
    "Stream tokens as they arrive for a longer generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6598dcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_prompt = (\n",
    "    'Generate three distinct premium brand names and for each a two-sentence tagline for '\n",
    "    'an eco-friendly smart home cleaning device brand. Emphasize sustainability, smart automation, and ease of use.'\n",
    ")\n",
    "print('Streaming start ->')\n",
    "for chunk in client.completions.create(model=MODEL, prompt=long_prompt, max_tokens=200, temperature=0.8, stream=True):\n",
    "    for c in chunk.choices:\n",
    "        text = getattr(c, 'text', '')\n",
    "        if text:\n",
    "            print(text, end='')\n",
    "print('\\n\\n<- Streaming end')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fb7f2b",
   "metadata": {},
   "source": [
    "## 7. Simple Helper Wrapper with Retry\n",
    "Basic utility to standardize calls & add retry-on-failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeea58c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List\n",
    "from openai import APIError, RateLimitError\n",
    "\n",
    "\n",
    "def generate(prompt: str, *, model: str = MODEL, n: int = 1, temperature: float = 0.7, max_tokens: int = 100, retries: int = 3, backoff: float = 1.5) -> List[str]:\n",
    "    attempt = 0\n",
    "    while True:\n",
    "        try:\n",
    "            resp = client.completions.create(\n",
    "                model=model, prompt=prompt, n=n, temperature=temperature, max_tokens=max_tokens)\n",
    "            return [c.text.strip() for c in resp.choices]\n",
    "        except (RateLimitError, APIError) as e:\n",
    "            attempt += 1\n",
    "            if attempt > retries:\n",
    "                raise\n",
    "            sleep_for = backoff ** attempt + random.random()\n",
    "            print(\n",
    "                f'Retry {attempt}/{retries} after error: {e}. Sleeping {sleep_for:.2f}s')\n",
    "            time.sleep(sleep_for)\n",
    "\n",
    "\n",
    "# New neutral sample prompt\n",
    "sample_prompt = 'List two differentiating value propositions for an eco-friendly smart home cleaning device.'\n",
    "results = generate(sample_prompt, n=2, temperature=0.6)\n",
    "for i, r in enumerate(results, 1):\n",
    "    print(f'Choice {i}: {r}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b8c37a",
   "metadata": {},
   "source": [
    "## 8. Recap & Next Steps\n",
    "You explored: model listing, baseline prompting, multiple candidates, penalties, token counting, streaming, and retries.\n",
    "Next ideas: add evaluation, cost estimation, structured extraction with stop sequences or JSON mode (if supported)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9514ba30",
   "metadata": {},
   "source": [
    "# OpenAI Completions Workshop Notebook\n",
    "\n",
    "End-to-end exploration of OpenAI *text completion* patterns: listing models, baseline prompting, refinement, multiple candidates, token awareness, penalties, and streaming.\n",
    "\n",
    "Environment requirement: `OPENAI_API_KEY` must be set (or a `.env` loaded earlier). Optional: set `OPENAI_MODEL` to override the default model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975bc434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & client initialization\n",
    "import os\n",
    "import time\n",
    "from openai import OpenAI\n",
    "\n",
    "API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "if not API_KEY:\n",
    "    raise EnvironmentError(\n",
    "        'OPENAI_API_KEY not set. Set it in your environment or .env file.')\n",
    "\n",
    "# Override by exporting OPENAI_MODEL if desired\n",
    "MODEL = os.getenv('OPENAI_MODEL', 'gpt-3.5-turbo-instruct')\n",
    "client = OpenAI(api_key=API_KEY)\n",
    "print(f'Using model: {MODEL}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090f098f",
   "metadata": {},
   "source": [
    "## 1. List Available Models\n",
    "Shows the raw list returned by the Models API (truncated for brevity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85adbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = client.models.list()\n",
    "print(f'Total models: {len(models.data)}')\n",
    "for m in models.data[:10]:  # show first 10\n",
    "    print(f'{m.id:40} | owned_by={getattr(m, 'owned_by', '?')}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a09f93",
   "metadata": {},
   "source": [
    "## 2. Baseline Completion\n",
    "Single prompt -> single completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6967cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    \"You are a branding assistant. Generate 5 concise, punchy tagline options for an \"\n",
    "    \"eco-friendly household cleaning spray named 'PureMist'. Each tagline should: \\n\"\n",
    "    \"1) Emphasize natural ingredients, \\n\"\n",
    "    \"2) Convey effectiveness, and \\n\"\n",
    "    \"3) Stay under 12 words.\\n\\n\"\n",
    "    \"Return them as a simple numbered list.\"\n",
    ")\n",
    "response = client.completions.create(\n",
    "    model=MODEL,\n",
    "    prompt=prompt,\n",
    "    max_tokens=120,\n",
    "    temperature=0.85,  # raised for creativity parity with script\n",
    "    n=1,\n",
    ")\n",
    "raw_text = response.choices[0].text.strip()\n",
    "lines = [l.strip() for l in raw_text.splitlines() if l.strip()]\n",
    "print(\"Model:\", MODEL)\n",
    "print(\"Prompt:\\n\" + prompt)\n",
    "print(\"\\nTagline Candidates:\")\n",
    "for line in lines:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4570efa4",
   "metadata": {},
   "source": [
    "## 3. Multiple Candidates & Temperature Sweep\n",
    "Generate several candidates to compare diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396ca8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "temps = [0.2, 0.7, 1.0]\n",
    "for t in temps:\n",
    "    resp = client.completions.create(\n",
    "        model=MODEL, prompt=prompt, max_tokens=100, n=3, temperature=t)\n",
    "    print(f'--- temperature={t} ---')\n",
    "    for i, choice in enumerate(resp.choices, 1):\n",
    "        print(f'[{i}] \\n{choice.text.strip()}\\n')\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ad6694",
   "metadata": {},
   "source": [
    "## 4. Presence & Frequency Penalties\n",
    "\n",
    "Beginner-friendly view:\n",
    "\n",
    "Large language models decide the next word based on probabilities learned from lots of text. Sometimes you want to nudge them to explore new wording or to stop them from repeating the same phrase.\n",
    "\n",
    "Two knobs help:\n",
    "\n",
    "- **presence_penalty**: “Have I said this token at least once?” If yes, gently lower its chance next time. This pushes the model to *introduce new ideas* or vocabulary. Think: diversity booster.\n",
    "- **frequency_penalty**: “How many times have I already used this token?” The more repeats, the stronger the down‑weight. This curbs *over-repetition* (e.g., the model saying the same word again and again).\n",
    "\n",
    "Plain analogy:\n",
    "- Presence = discourage re‑opening a door you already opened once.\n",
    "- Frequency = the more times you keep walking through the same door, the more resistance you feel.\n",
    "\n",
    "Typical ranges: 0.0 (off) to ~1.0 (strong). Start small (0.2–0.6). Increase if outputs feel stale or repetitive. Keep them at 0.0 if the task *needs* consistent terminology (e.g., legal definitions or code identifiers).\n",
    "\n",
    "Quick guidance:\n",
    "- Want more variety? Raise **presence_penalty** first.\n",
    "- Seeing exact phrase echoes (“green clean shine shine”)? Raise **frequency_penalty**.\n",
    "- If results become too off-topic, dial them back toward 0.\n",
    "\n",
    "Below we brute‑force a tiny grid just to *see* the qualitative difference—values are illustrative only.\n",
    "\n",
    "### Interpreting the Sample Output You Saw\n",
    "(You ran the loop over `(presence_penalty, frequency_penalty)` pairs: `(0.0,0.0)`, `(0.0,0.8)`, `(0.8,0.0)`, `(0.8,0.8)`.)\n",
    "\n",
    "**1. (0.0, 0.0)**  \n",
    "Baseline: Taglines are fine but you see recurring structures like “PureMist:” / “Gentle on…”. The model is free to reuse high‑probability phrases.\n",
    "\n",
    "**2. (0.0, 0.8)**  \n",
    "High *frequency* penalty alone discourages using the *same* high-frequency tokens repeatedly *within a single completion*. You still got eco / clean / nature themes (they're semantically on‑topic), but wording shifts a bit (“Powerful purity…”, “Eco‑friendly cleaning made easy.”). Two identical blocks appeared because the cell likely executed twice—the loop itself only yields that combination once.\n",
    "\n",
    "**3. (0.8, 0.0)**  \n",
    "High *presence* penalty pushes the model to introduce *new* tokens earlier. Result: slightly more structural variation (e.g., reordered phrasing “Nature's power, PureMist clean.”). Again, duplicate block = second execution of the cell, not the loop producing it twice.\n",
    "\n",
    "**4. (0.8, 0.8)**  \n",
    "Both penalties high. You saw fewer, longer taglines trending toward more descriptive phrasing. One list stopped after item 4 (blank #5). Causes can include:\n",
    "- Model chose to end early (it *thinks* output is done).\n",
    "- Hit `max_tokens=80` mid-enumeration (less likely but possible if earlier lines were long).\n",
    "- Penalties lowered probabilities of continuing the numbered pattern, increasing chance of early stop.\n",
    "\n",
    "**Why repeated groups?**  \n",
    "Your pasted output shows each non‑baseline configuration twice. That indicates the cell was likely run multiple times; the loop itself enumerates each pair only once.\n",
    "\n",
    "**Why blank #5?**  \n",
    "The model may have terminated before producing the fifth line. To mitigate:\n",
    "- Increase `max_tokens` (e.g., 120 → gives room if lines expand under penalties).\n",
    "- Make numbering *mandatory*: “Output exactly 5 lines numbered 1) .. 5). If you cannot, still produce placeholders.”\n",
    "- Add a stop sequence (e.g., `stop=[\"\\n6)\"]`) to encourage a clean finish after 5.\n",
    "\n",
    "### Practical Tweaks\n",
    "| Goal | Adjustment |\n",
    "|------|------------|\n",
    "| More lexical variety but still concise | Raise `presence_penalty` moderately (0.4–0.6) |\n",
    "| Reduce phrase echoes (“eco-friendly” every line) | Raise `frequency_penalty` first (0.4–0.7) |\n",
    "| Maintain strict count of items | Add explicit instruction + post-validate count |\n",
    "| Avoid truncated lists | Raise `max_tokens`; enforce completion via regex retry |\n",
    "\n",
    "### Simple Post-Validation Pattern\n",
    "After generation, if you expect exactly 5 items but count < 5, you can: (a) re-prompt asking only for the missing numbers, or (b) re-run with slightly lower penalties.\n",
    "\n",
    "Feel free to experiment by isolating just one penalty at a time to build intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71779bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for presence in [0.0, 0.8]:\n",
    "    for freq in [0.0, 0.8]:\n",
    "        resp = client.completions.create(\n",
    "            model=MODEL, prompt=prompt, max_tokens=80, temperature=0.7,\n",
    "            presence_penalty=presence, frequency_penalty=freq, n=1)\n",
    "        print(f'\\npresence_penalty={presence} frequency_penalty={freq}')\n",
    "        print(resp.choices[0].text.strip(), '\\n---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f87eb9",
   "metadata": {},
   "source": [
    "## 5. Token Counting (tiktoken)\n",
    "Estimate prompt token usage for budgeting / compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c9c4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import tiktoken\n",
    "    enc = tiktoken.get_encoding('cl100k_base')\n",
    "    token_count = len(enc.encode(prompt))\n",
    "    print('Prompt tokens:', token_count)\n",
    "except Exception as e:\n",
    "    print('tiktoken not installed or error occurred:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaee846",
   "metadata": {},
   "source": [
    "## 6. Streaming Responses\n",
    "Stream tokens as they arrive for a longer generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6598dcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_prompt = (\n",
    "    'Generate three distinct premium brand names and for each a two-sentence tagline for '\n",
    "    'an eco-friendly smart home cleaning device brand. Emphasize sustainability, smart automation, and ease of use.'\n",
    ")\n",
    "print('Streaming start ->')\n",
    "for chunk in client.completions.create(model=MODEL, prompt=long_prompt, max_tokens=200, temperature=0.8, stream=True):\n",
    "    for c in chunk.choices:\n",
    "        text = getattr(c, 'text', '')\n",
    "        if text:\n",
    "            print(text, end='')\n",
    "print('\\n\\n<- Streaming end')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fb7f2b",
   "metadata": {},
   "source": [
    "## 7. Simple Helper Wrapper with Retry\n",
    "Basic utility to standardize calls & add retry-on-failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeea58c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List\n",
    "from openai import APIError, RateLimitError\n",
    "\n",
    "\n",
    "def generate(prompt: str, *, model: str = MODEL, n: int = 1, temperature: float = 0.7, max_tokens: int = 100, retries: int = 3, backoff: float = 1.5) -> List[str]:\n",
    "    attempt = 0\n",
    "    while True:\n",
    "        try:\n",
    "            resp = client.completions.create(\n",
    "                model=model, prompt=prompt, n=n, temperature=temperature, max_tokens=max_tokens)\n",
    "            return [c.text.strip() for c in resp.choices]\n",
    "        except (RateLimitError, APIError) as e:\n",
    "            attempt += 1\n",
    "            if attempt > retries:\n",
    "                raise\n",
    "            sleep_for = backoff ** attempt + random.random()\n",
    "            print(\n",
    "                f'Retry {attempt}/{retries} after error: {e}. Sleeping {sleep_for:.2f}s')\n",
    "            time.sleep(sleep_for)\n",
    "\n",
    "\n",
    "# New neutral sample prompt\n",
    "sample_prompt = 'List two differentiating value propositions for an eco-friendly smart home cleaning device.'\n",
    "results = generate(sample_prompt, n=2, temperature=0.6)\n",
    "for i, r in enumerate(results, 1):\n",
    "    print(f'Choice {i}: {r}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b8c37a",
   "metadata": {},
   "source": [
    "## 8. Recap & Next Steps\n",
    "You explored: model listing, baseline prompting, multiple candidates, penalties, token counting, streaming, and retries.\n",
    "Next ideas: add evaluation, cost estimation, structured extraction with stop sequences or JSON mode (if supported)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
